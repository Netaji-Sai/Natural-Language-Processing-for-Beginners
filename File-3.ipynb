{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment-2**\n",
        "### **21BCE9182**\n",
        "### **Suru Netaji Sai**"
      ],
      "metadata": {
        "id": "LMxh9x0-GhNq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUcy9wBJfEN1",
        "outputId": "aa6db9ff-dc69-499e-ff50-e0bfada61dce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TBL train (fast) (seqs: 4623; tokens: 100554; tpls: 37; min score: 2; min acc: None)\n",
            "Finding initial useful rules...\n",
            "    Found 365168 useful rules.\n",
            "\n",
            "           B      |\n",
            "   S   F   r   O  |        Score = Fixed - Broken\n",
            "   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n",
            "   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n",
            "   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n",
            "   e   d   n   r  |  e\n",
            "------------------+-------------------------------------------------------\n",
            "26372637   015143  | None->NN if Pos:None@[1]\n",
            "12621262   0  30  | None->. if Pos:NN@[-3,-2,-1]\n",
            "11931193   0   3  | NN->AT if Word:the@[0] & Pos:NN@[1]\n",
            " 905 905   0   1  | NN->, if Word:,@[0]\n",
            " 568 598  30 338  | NN->IN if Pos:AT@[1] & Pos:NN@[2] & Word:the@[1]\n",
            " 516 574  58   2  | TO->IN if Pos:NN@[1,2]\n",
            " 427 427   0   5  | NN->CC if Word:and@[0]\n",
            " 379 379   0   0  | NN->AT if Word:a@[0]\n",
            " 359 359   0  21  | NN->IN if Word:of@[0]\n",
            " 243 243   0 118  | NN->TO if Word:to@[0]\n",
            " 242 242   0   9  | NN->IN if Word:in@[0]\n",
            " 187 187   0   0  | NN->'' if Word:''@[0]\n",
            " 163 176  13 116  | NN->VB if Pos:TO@[-1]\n",
            " 147 147   0   8  | NN->IN if Word:for@[0]\n",
            "  91  91   0   0  | NN->BEZ if Word:is@[0]\n",
            "  91  91   0  25  | NN->CS if Word:as@[0]\n",
            "  86  86   0   0  | NN->IN if Word:at@[0]\n",
            "  83  83   0   0  | NN->PPS if Word:he@[0]\n",
            "  81  81   0   0  | NN->`` if Word:``@[0]\n",
            "  77  77   0   1  | NN->PP$ if Word:his@[0]\n",
            "  76  76   0   0  | NN->BEDZ if Word:was@[0]\n",
            "  76  76   0   6  | NN->IN if Word:on@[0]\n",
            "  68  68   0  56  | NN->CS if Word:that@[0]\n",
            "  68  68   0   0  | NN->IN if Word:with@[0]\n",
            "  67  67   0   0  | NN->BE if Word:be@[0] & Pos:NN@[-1]\n",
            "  63  63   0   0  | NN->IN if Word:by@[0]\n",
            "  57  57   0   0  | NN->WPS if Word:who@[0]\n",
            "  54  54   0   0  | NN->PP$ if Word:their@[0] & Pos:NN@[1]\n",
            "  53  54   1   0  | NN->MD if Word:will@[0]\n",
            "  53  54   1   0  | IN->TO if Pos:VB@[1] & Pos:NN@[2]\n",
            "  52  52   0   0  | NN->AT if Word:an@[0] & Pos:NN@[1]\n",
            "  51  51   0   0  | NN->HVD if Word:had@[0]\n",
            "  50  50   0   0  | NN->WDT if Word:which@[0]\n",
            "  49  49   0   6  | IN->VB if Word:to@[-1]\n",
            "  48  50   2   0  | CS->QL if Word:as@[1,2,3]\n",
            "  48  53   5  21  | NN->VBN if Word:be@[-1]\n",
            "  47  47   0   0  | NN->* if Word:not@[0]\n",
            "  47  47   0  29  | NN->PPS if Word:it@[0]\n",
            "  46  46   0   0  | NN->IN if Word:from@[0]\n",
            "  46  46   0   0  | NN->PPSS if Word:they@[0]\n",
            "  45  45   0   0  | NN->DT if Word:this@[0]\n",
            "  44  44   0   1  | NN->VBD if Word:said@[0]\n",
            "  43  43   0   0  | NN->-- if Word:--@[0]\n",
            "  40  40   0   0  | NN->AP if Word:last@[0] & Pos:NN@[1]\n",
            "  38  38   0   0  | NN->BED if Word:were@[0]\n",
            "  37  37   0   0  | NN->CC if Word:but@[0]\n",
            "  36  36   0   0  | NN->) if Word:)@[0]\n",
            "  36  36   0   0  | NN->HVZ if Word:has@[0]\n",
            "  35  35   0   0  | NN->BER if Word:are@[0]\n",
            "  34  34   0   0  | NN->BEN if Word:been@[0]\n",
            "  32  32   0   2  | NN->CD if Word:one@[0]\n",
            "  32  32   0   0  | NN->HV if Word:have@[0]\n",
            "  31  31   0   0  | IN->CC if Word:and@[0] & Word:the@[1]\n",
            "  30  30   0   1  | NN->ABN if Word:all@[0]\n",
            "  30  30   0   0  | NN->MD if Word:would@[0]\n",
            "  30  30   0   1  | NN->RP if Word:up@[0]\n",
            "  30  30   0   4  | NN->VB if Pos:MD@[-1]\n",
            "  29  29   0   0  | NN->PP$ if Word:its@[0]\n",
            "  29  29   0  38  | NN->VBD if Pos:PPS@[-1]\n",
            "  28  28   0   0  | NN->AP if Word:other@[0]\n",
            "  28  28   0   0  | NN->CC if Word:or@[0]\n",
            "  26  37  11   4  | NN->NP if Word:Mrs.@[-3,-2,-1]\n",
            "  26  26   0   0  | NN->NNS if Word:years@[0]\n",
            "  26  26   0   0  | NN->PPO if Word:them@[0]\n",
            "  23  23   0   0  | NN->JJ if Word:new@[0] & Pos:NN@[1]\n",
            "  23  23   0   0  | NN->( if Word:(@[0]\n",
            "  23  23   0   4  | NN->RP if Word:out@[0]\n",
            "  22  22   0   0  | VB->BE if Word:be@[0] & Word:to@[-1]\n",
            "  22  22   0   0  | IN->CS if Word:as@[0] & Word:the@[1]\n",
            "  25  26   1   8  | NN->RB if Pos:QL@[-1] & Pos:CS@[1]\n",
            "  22  22   0   0  | NN->. if Word:;@[0] & Word:;@[1]\n",
            "  22  22   0   2  | NN->OD if Word:first@[0]\n",
            "  21  21   0   0  | NN->IN if Word:per@[0] & Pos:NN@[1]\n",
            "  21  21   0   0  | NN->JJ-TL if Word:New@[0] & Pos:NN@[1]\n",
            "  21  21   0   0  | NN->NN-TL if Word:President@[0]\n",
            "  21  22   1  11  | NN->VBN if Pos:HVD@[-1]\n",
            "  20  20   0   0  | NN->MD if Word:can@[0]\n",
            "  21  61  40 140  | NN->NP if Pos:,@[-1] & Pos:NN@[1]\n",
            "  20  20   0   0  | NN->PPO if Word:him@[0]\n",
            "  20  74  54 186  | NN->NP if Word:to@[-1]\n",
            "  19  20   1   6  | NN->NP-TL if Word:New@[-1]\n",
            "  18  18   0   0  | NN->PPSS if Word:I@[0]\n",
            "  17  17   0   0  | IN->CS if Word:that@[0] & Word:the@[1]\n",
            "  17  19   2   5  | NN->NP if Word:Dr.@[-3,-2,-1]\n",
            "  18  31  13  25  | NN->VBN if Word:was@[-1]\n",
            "  17  17   0   2  | NN->NP if Word:James@[-3,-2,-1]\n",
            "  17  17   0   9  | NN->AP if Word:more@[0]\n",
            "  17  17   0   0  | NN->CD if Word:two@[0]\n",
            "  17  17   0  13  | NN->CS if Word:than@[0]\n",
            "  17  17   0   0  | NN->NNS if Word:members@[0]\n",
            "  17  17   0   0  | NN->WDT if Word:what@[0]\n",
            "  17  17   0  16  | NN->VBD if Word:who@[-1]\n",
            "  16  16   0   0  | NN->NP if Word:Mrs.@[0] & Pos:NP@[1]\n",
            "  16  16   0   0  | NN->: if Word::@[0]\n",
            "  16  16   0   2  | NN->IN if Word:against@[0]\n",
            "  16  16   0   0  | NN->NNS if Word:people@[0]\n",
            "  15  18   3   0  | IN->RP if Word:in@[0] & Pos:IN@[1]\n",
            "  15  15   0   0  | NN->CS if Word:if@[0]\n",
            "  15  15   0   0  | NN->IN if Word:into@[0]\n",
            "  15  17   2  14  | NN->VBN if Word:been@[-1]\n",
            "  14  14   0   0  | NN->. if Word:?@[0] & Word:?@[1]\n",
            "  14  14   0   3  | NN->NP if Word:John@[-2,-1]\n",
            "  14  15   1   1  | CS->WPS if Word:that@[0] & Pos:MD@[1]\n",
            "  14  14   0   0  | NN->PP$ if Word:our@[0] & Pos:NN@[1]\n",
            "  14  14   0   0  | NN->VBN-TL if Word:United@[0] & Pos:NN@[1]\n",
            "  14  14   0   0  | TO->IN if Word:to@[0] & Pos:AT@[1]\n",
            "  14  14   0   0  | NN->AT if Word:no@[0]\n",
            "  14  14   0   1  | NN->JJ if Word:own@[0]\n",
            "  14  14   0   0  | NN->NR if Word:yesterday@[0]\n",
            "  14  14   0   3  | NN->NP if Word:Mr.@[-1]\n",
            "  13  13   0   0  | NN->DTI if Word:any@[0]\n",
            "  13  13   0   0  | NN->NN-TL if Word:University@[0]\n",
            "  13  13   0   0  | NN->NR if Word:today@[0]\n",
            "  13  13   0   0  | NN->PPS if Word:she@[0]\n",
            "  13  13   0   0  | NN->RB if Word:here@[0]\n",
            "  13  20   7   7  | NN->MD if Word:be@[1]\n",
            "  12  12   0   1  | NN->CD if Word:cent@[1,2]\n",
            "  12  12   0   0  | NN->BEG if Word:being@[0]\n",
            "  12  12   0   0  | NN->CD if Word:four@[0]\n",
            "  12  12   0   0  | NN->CD if Word:three@[0]\n",
            "  12  12   0   0  | NN->DTI if Word:some@[0]\n",
            "  12  12   0   0  | NN->IN if Word:without@[0]\n",
            "  12  14   2   0  | NN->JJ if Word:good@[0]\n",
            "  12  12   0   0  | NN->NP if Word:Mr.@[0]\n",
            "  12  12   0   0  | NN->NR if Word:Monday@[0]\n",
            "  12  12   0   0  | NN->WRB if Word:where@[0]\n",
            "  12  12   0   0  | NN->VBN if Word:being@[-1]\n",
            "  12  12   0   1  | IN->NN if Pos:AT@[-1] & Pos:AT@[1]\n",
            "  12  12   0  22  | NN->VB if Pos:PPSS@[-1]\n",
            "  11  14   3  10  | NN->NP if Word:President@[-2,-1]\n",
            "  11  12   1   0  | JJ->ABL if Word:such@[0] & Pos:AT@[1]\n",
            "  11  11   0   0  | NN->JJ-TL if Word:National@[0] & Pos:NN@[1]\n",
            "  11  11   0   3  | NN->EX if Word:there@[0]\n",
            "  11  11   0   7  | NN->IN if Word:after@[0]\n",
            "  11  12   1   0  | NN->NN-TL if Word:City@[0]\n",
            "  11  11   0   0  | NN->NN-TL if Word:League@[0]\n",
            "  11  11   0   0  | NN->NNS if Word:children@[0]\n",
            "  11  11   0   2  | NN->NP if Word:U.S.@[0]\n",
            "  11  11   0   0  | NN->PPSS if Word:we@[0]\n",
            "  11  11   0   0  | NN->RB if Word:also@[0]\n",
            "  11  11   0   6  | NN->VBD if Word:made@[0]\n",
            "  11  11   0   3  | NN->NNS-TL if Word:United@[-1]\n",
            "  11  14   3   1  | NN->VBN if Word:has@[-1]\n",
            "  11  11   0   3  | NN->NP if Pos:,@[-1] & Pos:VBD@[1]\n",
            "  11  11   0   0  | NP-TL->NP if Pos:AT@[-1] & Pos:NN@[1]\n",
            "  10  55  45  42  | CS->DT if Word:that@[0] & Pos:NN@[1]\n",
            "  22  35  13  21  | DT->WPS if Word:that@[0] & Pos:NN@[-1]\n",
            "  10  10   0   1  | NN->CS if Word:because@[0]\n",
            "  10  10   0   0  | NN->DT if Word:each@[0]\n",
            "  10  10   0   5  | NN->IN if Word:about@[0]\n",
            "  10  10   0   0  | NN->NR if Word:Saturday@[0]\n",
            "  10  10   0   4  | NN->PP$ if Word:her@[0]\n",
            "  10  10   0   0  | NN->RB if Word:now@[0]\n",
            "  10  11   1   3  | IN->VB if Pos:MD@[-1] & Pos:AT@[1]\n",
            "   9  11   2   0  | CS->IN if Word:than@[0] & Word:more@[-1]\n",
            "   9   9   0   0  | PPO->PPS if Word:it@[0] & Word:said@[-1]\n",
            "   9   9   0   4  | CS->WPS if Word:that@[0] & Word:is@[1]\n",
            "   9   9   0   0  | IN->TO if Word:to@[0] & Word:be@[1]\n",
            "   9   9   0   0  | IN->TO if Word:to@[0] & Word:the@[2]\n",
            "   9   9   0   1  | NN->CD if Word:p.m.@[1,2]\n",
            "   9   9   0   0  | NN->NP if Word:Chairman@[-2,-1]\n",
            "   9   9   0   0  | IN-TL->IN if Word:of@[0] & Pos:NN@[1]\n",
            "   9   9   0   3  | NN->JJ if Word:past@[0] & Pos:NN@[1]\n",
            "   9   9   0   0  | NN->QL if Word:too@[0] & Pos:NN@[1]\n",
            "   9   9   0   0  | NN->RB if Word:p.m.@[0] & Pos:CD@[-1]\n",
            "   9   9   0   0  | NN->DTS if Word:those@[0]\n",
            "   9   9   0   6  | NN->IN if Word:over@[0]\n",
            "   9   9   0   5  | NN->JJ if Word:American@[0]\n",
            "   9   9   0   0  | NN->JJ if Word:possible@[0]\n",
            "   9   9   0   1  | NN->NN-TL if Word:Dr.@[0]\n",
            "   9   9   0   1  | NN->NP if Word:Dallas@[0]\n",
            "   9   9   0   0  | NN->NR if Word:Sunday@[0]\n",
            "   9   9   0   0  | NN->RB if Word:again@[0]\n",
            "   9   9   0   0  | NN->RB if Word:then@[0]\n",
            "   8   8   0   0  | NN->NP if Word:Robert@[-2,-1]\n",
            "   8  10   2   3  | NN->NP-TL if Word:North@[-2,-1]\n",
            "   8   9   1   9  | NN->VB if Word:can@[-2,-1]\n",
            "   8   8   0   0  | NN->AT if Word:every@[0] & Pos:NN@[1]\n",
            "   8   8   0   0  | NN->JJ if Word:general@[0] & Pos:NN@[1]\n",
            "   8   8   0   0  | NN->JJ-TL if Word:Democratic@[0] & Pos:NN@[1]\n",
            "   8   8   0   0  | WPS->CS if Word:that@[0] & Pos:AT@[1]\n",
            "   8   8   0   0  | NN->NN-TL if Word:Committee@[0] & Pos:NN@[-1]\n",
            "   8   9   1   0  | PPS->PPO if Word:it@[0] & Pos:IN@[-1]\n",
            "  10  10   0   0  | IN->CS if Pos:PPS@[1]\n",
            "   8   8   0   0  | .->: if Word::@[0]\n",
            "   8   8   0   0  | NN->CD if Word:10@[0]\n",
            "   8   8   0   0  | NN->CD if Word:1960@[0]\n",
            "   8   8   0   0  | NN->CD if Word:million@[0]\n",
            "   8   8   0   1  | NN->JJ if Word:big@[0]\n",
            "   8   8   0   2  | NN->JJ if Word:such@[0]\n",
            "   8   8   0   0  | NN->NN-TL if Word:Church@[0]\n",
            "   8   8   0   0  | NN->NN-TL if Word:Senate@[0]\n",
            "   8   8   0   0  | NN->NNS if Word:laws@[0]\n",
            "   8   8   0   0  | NN->NNS if Word:men@[0]\n",
            "   8   8   0   0  | NN->NNS if Word:persons@[0]\n",
            "   8   8   0   0  | NN->NNS if Word:schools@[0]\n",
            "   8   8   0   0  | NN->NR if Word:Tuesday@[0]\n",
            "   8   8   0   0  | NN->PPLS if Word:themselves@[0]\n",
            "   8   8   0   0  | NN->RB if Word:ago@[0]\n",
            "   8   8   0   0  | NN->RB if Word:still@[0]\n",
            "   8   8   0   0  | NN->WRB if Word:when@[0]\n",
            "   8  10   2  12  | NN->VB if Word:not@[-1]\n",
            "   8  11   3  11  | NN->VBN if Word:were@[-1]\n",
            "   7   7   0   0  | NN->AP if Word:same@[0] & Word:the@[-1]\n",
            "   7   7   0   0  | CS->WPS if Word:that@[0] & Word:has@[1]\n",
            "   7   7   0   0  | IN->-- if Word:--@[0] & Word:the@[1]\n",
            "   7   7   0   0  | IN->WDT if Word:which@[0] & Word:the@[1]\n",
            "   8   9   1   0  | IN->RP if Word:on@[0] & Pos:IN@[1]\n",
            "   7   7   0   0  | NN->NP if Word:George@[-2,-1]\n",
            "   7   7   0   0  | NN->NP if Word:W.@[-2,-1]\n",
            "   7   7   0   8  | NN->VB if Word:could@[-2,-1]\n",
            "   8   8   0   0  | VB->DO if Word:do@[0]\n",
            "   7   7   0   0  | NN->NP if Word:Jr.@[1,2,3]\n",
            "   7   8   1   0  | IN->RB if Word:about@[0] & Pos:CD@[1]\n",
            "   7   7   0   0  | NN->JJ if Word:federal@[0] & Pos:NN@[1]\n",
            "   7   7   0   0  | NN->JJ if Word:private@[0] & Pos:NN@[1]\n",
            "   7   7   0   0  | NN->JJ-TL if Word:North@[0] & Pos:NP-TL@[1]\n",
            "   7   7   0   0  | NN->NN-TL if Word:Communist@[0] & Pos:NN@[1]\n",
            "   7   7   0   0  | NN->PP$ if Word:my@[0] & Pos:NN@[1]\n",
            "   7   7   0   0  | NN->NN-TL if Word:Service@[0] & Pos:NN@[-1]\n",
            "   7   7   0   0  | NN->' if Word:'@[0]\n",
            "   7   7   0   0  | NN->AP if Word:few@[0]\n",
            "   7   7   0   1  | NN->CD if Word:1@[0]\n",
            "   7   7   0   0  | NN->CD if Word:1961@[0]\n",
            "   7   7   0   0  | NN->DOD if Word:did@[0]\n",
            "   7   7   0   0  | NN->IN if Word:through@[0]\n",
            "   7   7   0   0  | NN->JJ if Word:anti-trust@[0]\n",
            "   7   7   0   0  | NN->JJ if Word:certain@[0]\n",
            "   7   7   0   0  | NN->JJ if Word:medical@[0]\n",
            "   7   7   0   0  | NN->JJ if Word:necessary@[0]\n",
            "   7   7   0   0  | NN->NN-TL if Word:Club@[0]\n",
            "   7   7   0   0  | NN->NN-TL if Word:Coast@[0]\n",
            "   7   8   1   0  | NN->NN-TL if Word:Council@[0]\n",
            "   8  12   4  15  | NN->NP if Pos:IN@[-1] & Pos:NN-TL@[-2]\n",
            "   7   7   0   0  | NN->NN-TL if Word:County@[0]\n",
            "   7   7   0   0  | NN->NN-TL if Word:House@[0]\n",
            "   7   7   0   2  | NN->NN-TL if Word:St.@[0]\n",
            "   8   8   0  11  | NN->NP-TL if Pos:NN-TL@[1] & Pos:NN@[2]\n",
            "   7   7   0   0  | NN->NNS if Word:sales@[0]\n",
            "   7   7   0   0  | NN->NNS if Word:yards@[0]\n",
            "   7   7   0   3  | NN->NP if Word:Chicago@[0]\n",
            "   7   7   0   0  | NN->RB if Word:ever@[0]\n",
            "   7   7   0   3  | NN->JJ if Word:too@[-1]\n",
            "   7  11   4   0  | NN->NP if Pos:NP@[1] & Pos:,@[2]\n",
            "   6   6   0   0  | IN->RP if Word:in@[0] & Word:bring@[-1]\n",
            "   6   6   0   0  | IN->BEDZ if Word:was@[0] & Word:the@[1]\n",
            "   6   6   0   0  | IN->WRB if Word:when@[0] & Word:the@[1]\n",
            "   6   6   0   0  | JJ->JJ-TL if Word:American@[0] & Word:League@[1]\n",
            "   6   6   0   0  | RB->IN if Word:around@[0] & Word:the@[1]\n",
            "   6   6   0   0  | NP->NP-TL if Word:University@[1,2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-89677627f2e3>:19: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  print(tagger.evaluate(brown.tagged_sents(categories='fiction')[:]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7058024763462213\n",
            "[('My', 'PP$'), ('name', 'NN'), ('is', 'BEZ'), ('Ujwala', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "from nltk.tag.brill import *\n",
        "import nltk.tag.brill_trainer as bt\n",
        "from nltk.corpus import brown\n",
        "\n",
        "Template._cleartemplates()\n",
        "templates = fntbl37()\n",
        "\n",
        "tagged_sentences = brown.tagged_sents(categories='news')\n",
        "tagged_sentences = tagged_sentences[:]\n",
        "\n",
        "tagger = nltk.tag.BigramTagger(tagged_sentences)\n",
        "\n",
        "tagger = bt.BrillTaggerTrainer(tagger, templates, trace=3)\n",
        "tagger = tagger.train(tagged_sentences, max_rules=250)\n",
        "\n",
        "print(tagger.evaluate(brown.tagged_sents(categories='fiction')[:]))\n",
        "print(tagger.tag(nltk.word_tokenize(\"My name is Ujwala.\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "chunker = nltk.data.load('chunkers/maxent_ne_chunker/english_ace_multiclass.pickle')\n",
        "maxEnt = chunker._tagger.classifier()\n",
        "\n",
        "sentence = 'I love Apple products'\n",
        "\n",
        "def ne_report(sentence, report_all=False):\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tokens = nltk.pos_tag(tokens)\n",
        "    tags = []\n",
        "    print(tokens)\n",
        "    for i in range(0, len(tokens)):\n",
        "        featureset = chunker._tagger.feature_detector(tokens, i, tags)\n",
        "        tag = chunker._tagger.choose_tag(tokens, i, tags)\n",
        "        if tag != 'O' or report_all:\n",
        "            print('\\nExplanation on why the word \\'' + tokens[i][0] + '\\' was tagged:')\n",
        "            featureset = chunker._tagger.feature_detector(tokens, i, tags)\n",
        "            maxEnt.explain(featureset)\n",
        "        tags.append(tag)\n",
        "    print(tokens)\n",
        "    print(maxEnt)\n",
        "\n",
        "ne_report(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF9YMApRglOv",
        "outputId": "cbe12f08-0938-4eb2-f5a4-bba9ec4db06f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('love', 'VBP'), ('Apple', 'NNP'), ('products', 'NNS')]\n",
            "[('I', 'PRP'), ('love', 'VBP'), ('Apple', 'NNP'), ('products', 'NNS')]\n",
            "<ConditionalExponentialClassifier: 13 labels, 149589 features>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk import word_tokenize,pos_tag\n",
        "text = \"Big Data is a collection of data that is huge in volume, yet growing exponentiall y with time.\"\n",
        "tokens = word_tokenize(text)\n",
        "tag=pos_tag(tokens)\n",
        "print(tag)\n",
        "ne_tree = nltk.ne_chunk(tag)\n",
        "print(ne_tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgXyHD98g8cQ",
        "outputId": "29ea7c48-be32-453f-f9ca-7472b5bb6fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Big', 'NNP'), ('Data', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('collection', 'NN'), ('of', 'IN'), ('data', 'NNS'), ('that', 'WDT'), ('is', 'VBZ'), ('huge', 'JJ'), ('in', 'IN'), ('volume', 'NN'), (',', ','), ('yet', 'RB'), ('growing', 'VBG'), ('exponentiall', 'JJ'), ('y', 'NN'), ('with', 'IN'), ('time', 'NN'), ('.', '.')]\n",
            "(S\n",
            "  Big/NNP\n",
            "  Data/NNP\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  collection/NN\n",
            "  of/IN\n",
            "  data/NNS\n",
            "  that/WDT\n",
            "  is/VBZ\n",
            "  huge/JJ\n",
            "  in/IN\n",
            "  volume/NN\n",
            "  ,/,\n",
            "  yet/RB\n",
            "  growing/VBG\n",
            "  exponentiall/JJ\n",
            "  y/NN\n",
            "  with/IN\n",
            "  time/NN\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "text = \"Renard series first proposed by French Army engineer Charles Renard. In March 201 7, the decision to introduce ₹200 notes was taken by the Reserve Bank of India with the c onsultation of the Ministry of Finance.\"\n",
        "sentence = nltk.sent_tokenize(text)\n",
        "for sent in sentence: print(nltk.pos_tag(nltk.word_tokenize(sent)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJqmF-RQhtcH",
        "outputId": "86df6c53-8f38-4920-a35d-50805a1fbbe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Renard', 'NNP'), ('series', 'NN'), ('first', 'RB'), ('proposed', 'VBN'), ('by', 'IN'), ('French', 'JJ'), ('Army', 'NNP'), ('engineer', 'NN'), ('Charles', 'NNP'), ('Renard', 'NNP'), ('.', '.')]\n",
            "[('In', 'IN'), ('March', 'NNP'), ('201', 'CD'), ('7', 'CD'), (',', ','), ('the', 'DT'), ('decision', 'NN'), ('to', 'TO'), ('introduce', 'VB'), ('₹200', 'JJ'), ('notes', 'NNS'), ('was', 'VBD'), ('taken', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('Reserve', 'NNP'), ('Bank', 'NNP'), ('of', 'IN'), ('India', 'NNP'), ('with', 'IN'), ('the', 'DT'), ('c', 'JJ'), ('onsultation', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Ministry', 'NNP'), ('of', 'IN'), ('Finance', 'NNP'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "PP -> P NP\n",
        "NP -> Det N | Det N PP | 'I'\n",
        "VP -> V NP | VP PP\n",
        "Det -> 'an' | 'my'\n",
        "N -> 'elephant' | 'pajamas'\n",
        "V -> 'shot'\n",
        "P -> 'in'\"\"\")\n"
      ],
      "metadata": {
        "id": "o_z-hzQhh5_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groucho_grammar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JbRwhhtiBiQ",
        "outputId": "3cd9a953-afba-42fc-81f0-18bee5482991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Grammar with 13 productions>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(groucho_grammar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "8hStgw74iCIq",
        "outputId": "8695229e-e720-408c-b731-3e4650f8cd9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.grammar.CFG"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>nltk.grammar.CFG</b><br/>def __init__(start, productions, calculate_leftcorners=True)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/nltk/grammar.py</a>A context-free grammar.  A grammar consists of a start state and\n",
              "a set of productions.  The set of terminals and nonterminals is\n",
              "implicitly specified by the productions.\n",
              "\n",
              "If you need efficient key-based access to productions, you\n",
              "can use a subclass to implement it.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 445);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.grammar import *\n",
        "groucho_grammar.productions(lhs=Nonterminal(\"NP\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJCz_IvZiD3_",
        "outputId": "3d47b562-4068-4f57-969c-8bc9362d8de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[NP -> Det N, NP -> Det N PP, NP -> 'I']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groucho_grammar.productions(rhs=Nonterminal(\"Det\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VukMVjsUiH6b",
        "outputId": "24ec7fd8-5252-4123-be0a-0ebc878fa2c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[NP -> Det N, NP -> Det N PP]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pp = groucho_grammar.productions(rhs=Nonterminal(\"Det\"))\n",
        "pp[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmmfSwFHiKY6",
        "outputId": "4d7299bd-acea-489d-ef44-27c19f87266b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NP -> Det N"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pp[0].lhs()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXOkQZaOiK9o",
        "outputId": "d6031e1e-9772-4bb4-d7ba-7f272d0244f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NP"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pp[0].rhs()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMj_9EXeiMyD",
        "outputId": "21ac798c-3850-462c-f36e-167e470f898d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Det, N)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toy_pcfg2 = PCFG.fromstring(\"\"\"\n",
        "S\t->\tNP VP\t[1.0]\n",
        "VP\t->\tV NP\t[.59]\n",
        "VP\t->\tV\t[.40]\n",
        "VP\t->\tVP PP\t[.01]\n",
        "NP\t->\tDet N\t[.41]\n",
        "NP\t->\tName\t[.28]\n",
        "NP\t->\tNP PP\t[.31]\n",
        "PP\t->\tP NP\t[1.0]\n",
        "V\t->\t'saw'\t[.21]\n",
        "V\t->\t'ate'\t[.51]\n",
        "V\t->\t'ran'\t[.28]\n",
        "N\t->\t'boy'\t[.11]\n",
        "N\t->\t'cookie'\t[.12]\n",
        "N\t->\t'table'\t[.13]\n",
        "N\t->\t'telescope'\t[.14]\n",
        "N\t->\t'hill'\t[.5]\n",
        "Name\t->\t'Jack'\t[.52]\n",
        "Name\t->\t'Bob'\t[.48]\n",
        "P\t->\t'with'\t[.61]\n",
        "P\t->\t'under'\t[.39]\n",
        "Det\t->\t'the'\t[.41]\n",
        "Det\t->\t'a'\t[.31]\n",
        "Det\t->\t'my'\t[.28]\t\"\"\")\n"
      ],
      "metadata": {
        "id": "HEI2EeYbiOgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = \"Jack saw Bob with my cookie\".split()\n",
        "grammar = toy_pcfg2\n",
        "print(grammar)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oinfk8u_icU5",
        "outputId": "01b978a7-d8cd-42c3-8372-f43fa7a661ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grammar with 23 productions (start state = S)\n",
            "    S -> NP VP [1.0]\n",
            "    VP -> V NP [0.59]\n",
            "    VP -> V [0.4]\n",
            "    VP -> VP PP [0.01]\n",
            "    NP -> Det N [0.41]\n",
            "    NP -> Name [0.28]\n",
            "    NP -> NP PP [0.31]\n",
            "    PP -> P NP [1.0]\n",
            "    V -> 'saw' [0.21]\n",
            "    V -> 'ate' [0.51]\n",
            "    V -> 'ran' [0.28]\n",
            "    N -> 'boy' [0.11]\n",
            "    N -> 'cookie' [0.12]\n",
            "    N -> 'table' [0.13]\n",
            "    N -> 'telescope' [0.14]\n",
            "    N -> 'hill' [0.5]\n",
            "    Name -> 'Jack' [0.52]\n",
            "    Name -> 'Bob' [0.48]\n",
            "    P -> 'with' [0.61]\n",
            "    P -> 'under' [0.39]\n",
            "    Det -> 'the' [0.41]\n",
            "    Det -> 'a' [0.31]\n",
            "    Det -> 'my' [0.28]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.parse import pchart\n",
        "\n",
        "parser = pchart.InsideChartParser(grammar)\n",
        "for t in parser.parse(tokens):\n",
        "    print(t)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyinNnB5iea1",
        "outputId": "26c3c1c5-74e7-4957-9dbe-2bc3732d0fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP (Name Jack))\n",
            "  (VP\n",
            "    (V saw)\n",
            "    (NP\n",
            "      (NP (Name Bob))\n",
            "      (PP (P with) (NP (Det my) (N cookie)))))) (p=6.31607e-06)\n",
            "(S\n",
            "  (NP (Name Jack))\n",
            "  (VP\n",
            "    (VP (V saw) (NP (Name Bob)))\n",
            "    (PP (P with) (NP (Det my) (N cookie))))) (p=2.03744e-07)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = pchart.RandomChartParser(grammar)\n",
        "for t in parser.parse(tokens):\n",
        "    print(t)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luKEZVHMigtd",
        "outputId": "8c139170-30d0-48b4-bf15-e4dcabe4451d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP (Name Jack))\n",
            "  (VP\n",
            "    (V saw)\n",
            "    (NP\n",
            "      (NP (Name Bob))\n",
            "      (PP (P with) (NP (Det my) (N cookie)))))) (p=6.31607e-06)\n",
            "(S\n",
            "  (NP (Name Jack))\n",
            "  (VP\n",
            "    (VP (V saw) (NP (Name Bob)))\n",
            "    (PP (P with) (NP (Det my) (N cookie))))) (p=2.03744e-07)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = pchart.UnsortedChartParser(grammar)\n",
        "for t in parser.parse(tokens):\n",
        "    print(t)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnIeL8ovikWQ",
        "outputId": "9a7f1802-9fda-4995-b917-36e422a4476e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP (Name Jack))\n",
            "  (VP\n",
            "    (V saw)\n",
            "    (NP\n",
            "      (NP (Name Bob))\n",
            "      (PP (P with) (NP (Det my) (N cookie)))))) (p=6.31607e-06)\n",
            "(S\n",
            "  (NP (Name Jack))\n",
            "  (VP\n",
            "    (VP (V saw) (NP (Name Bob)))\n",
            "    (PP (P with) (NP (Det my) (N cookie))))) (p=2.03744e-07)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = pchart.LongestChartParser(grammar)\n",
        "for t in parser.parse(tokens):\n",
        "    print(t)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OslGlJVzilMB",
        "outputId": "565b2e43-1125-4343-fefe-cb20deb8e56a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP (Name Jack))\n",
            "  (VP\n",
            "    (V saw)\n",
            "    (NP\n",
            "      (NP (Name Bob))\n",
            "      (PP (P with) (NP (Det my) (N cookie)))))) (p=6.31607e-06)\n",
            "(S\n",
            "  (NP (Name Jack))\n",
            "  (VP\n",
            "    (VP (V saw) (NP (Name Bob)))\n",
            "    (PP (P with) (NP (Det my) (N cookie))))) (p=2.03744e-07)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = pchart.InsideChartParser(grammar, beam_size = len(tokens)+1)\n",
        "for t in parser.parse(tokens):\n",
        "    print(t)\n"
      ],
      "metadata": {
        "id": "T43Wy-IFinHq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
